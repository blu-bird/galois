\documentclass[12pt]{scrartcl}
\usepackage{blubird}
% has options [nodate, nosans, nofancy, nocolor, code]

% box setup - should be moved to somewhere nicer
\mdfsetup{
	roundcorner = 2pt,
	linewidth = 1pt,
	innertopmargin = 0.5em,
	innerbottommargin = 1em,
	frametitlefont = \bfseries,
}

% TITLE
\title{Galois Theory -- Historical Perspective}
\author{Bryan Lu}
\date{May 2022} % do not use if [nodate] option enabled

\begin{document}
\maketitle

\setcounter{section}{-1}
\section{Foreword}
These are some notes on Galois theory following H. M. Edwards' \textit{Galois Theory}. Galois theory is an overlooked subject in Cornell's algebra sequence -- it's only given a passing mention in MATH 4340, the honors abstract algebra course, and it's kind of a shame it's treated this way. At its heart is a nice connection between fields and groups and I feel like it's a big part of why one would want to study fields in the first place. I also think Galois theory is important in leveling up an understanding of number theory as well as dealing with fields and number systems in general beyond just the integers, so I think it's useful in algebraic NT...? I really don't know much more in this regard so I should stop talking lol but those are my thoughts on why it's important, at least.

I also think that there's a lot of historical mystique around the field as well -- a lot of people know the story of how Evariste Galois was shot and killed in a duel at age 21, and the night before he died he feverishly wrote down the groundwork for this really amazing theory. What a story, right? I kinda like Edwards' book because the smoke and mirrors are cleared here -- you get the benefit of the historical context in which Galois was working, and also understand the angle at which he was coming from when he wrote all that stuff without losing the modern interpretation of the results. I don't think this is a very standard treatment of the subject at all, but I think it's fun nonetheless and will do some mixing in with Dummit and Foote as needed.

Some prerequisites -- while you don't technically need to have any background in algebra to understand the book, I will assume familiarity with group theory in these notes. These notes are a distillation of the important developments in the field's history and while the book does go out of its way to also explain groups and stuff, I won't. There's also the whole story about solving the cubic/quartic with Cardano and Tartaglia and Ferrari and del Ferro which is commonly used as historical setup for Abel-Ruffini, a theorem often discussed in a treatment Galois theory -- if you're interested in the story, I won't rehash it here as so many people do it, but if you don't know about it you should find out! Veritasium has a really nice popular depiction of it and he does a much better job than I ever could so I'll assume you'll get the story from elsewhere and I won't have to talk about it.

\section{History of Solving Polynomial Equations}
A brief history of solving polynomial equations (of degree higher than 1):
\begin{itemize}
	\item The Babylonians knew how to solve quadratics! This is akin to Po-Shen Loh's method of solving quadratics today -- conceiving it instead as a puzzle in which you have to figure out two numbers (the roots of a quadratic) given only their sum and product.
	\item Not a lot of progress on cubics and quartics until the aforementioned developments by del Ferro/Cardano/Tartaglia in the 16th century -- there's a surprising amount of drama and tea here. The main ideas:
	      \begin{itemize}
		      \item Depressing the cubic -- take cubics of the form $x^3 + ax^2 + bx + c$ and introduce the change of variables $y = x - \frac a 3$, which eliminates the quadratic term. Leaves you with an equation of the form $y^3 + py + q$.
		      \item Guess that $y = m - n$. Substituting, you can see that a good heuristic for the solution is that $3mn = p$, $m^3 - n^3 + q = 0$. Note that $27m^6 - 27 m^3 n^3 + 27 m^3 q = 0$ as well, but $27 m^3 n^3 = p^3$, so you actually have a quadratic in $m^3$ now, allowing you to solve for $m$ and $n$ now, and then unwinding to get $y$ and then $x$.
	      \end{itemize}
	\item Quartics would fall very shortly after due to Ferrari. This is more undisputed I think? Again, just the main ideas:
	      \begin{itemize}
		      \item Depress the quartic now -- introduce the change of variables $y = x - \frac a 4$ for the quartic $x^4 + ax^3 + bx^2 + cx + d$, which leaves a new quartic $y^4 + py^2 + qy + r$.
		      \item Introduce a new variable $z$, and consider $(y^2 + z)^2 = y^4 + 2 y^2z + z^2 = (-p + 2z)y^2 - qy - r + z^2$. Heuristically, we want to be able to take the square root of the RHS. This means that this quadratic in $y$ actually has to have a single root, so this happens when $q^2 - 4(-p + 2z)(z^2 - r) = 0$. This gives a cubic in $z$ that can be solved!
		      \item Solve for $z$ using the procedure for a cubic. If $z$ is now chosen appropriately, we get that $(y^2 + z) = \pm \sqrt{-p + 2z} \left(y - \frac q{2(-p + 2z)} \right)$, which gives a quadratic that can now be solved.
	      \end{itemize}
	\item Abel shows in the 19th century (c. 1820) that it's impossible to solve qunitics or higher in general form in the same way as above. (This proof is not based in Galois theory, but uses some of the techniques mentioned in future sections.)

	      Note that this is about a 300-year gap without seemingly much progress on this problem! What were people up to in that time?
\end{itemize}

\subsection{Fundamental Theorem on Symmetric Polynomials, Vieta's Formulas, Newton's Sums}
One of the most important developments in algebra (for us) in the time between the time of the Great Italian War of Cubics and Quartics and Abel's big contribution in the early 1800s is a development in the theory of symmetric polynomials.

First, I feel like I have to mention Francois Viete here, along with Vieta's formulas -- discovered in the late 16th century. I do need to provide a definition first --
\begin{definition}
	The \textbf{$k$th elementary symmetric polynomial} on $n$ variables $x_1, x_2, \dots, x_n$, $e_k(x_1, x_2, \dots, x_n)$ is the sum of all $\binom n k$ terms $x_{i_1} x_{i_2} \dots x_{i_k}$ ($i_1 < i_2 < \dots < i_k$) where all of the $i_j \in [n]$.
\end{definition}
Viete figured out how to express elementary symmetric polynomials in the roots of a polynomial in terms of the coefficients of that polynomial:
\begin{theorem}[Vieta's Formulas]
	If a polynomial $a_n x^n + \dots + a_1 x + a_0$ has roots $r_1, r_2, \dots, r_n$, then the elementary symmetric polynomials are expressible in terms of the roots. In particular, $e_{k}(r_1, r_2, \dots, r_n) = (-1)^{k} \frac{a_{n-k}}{a_n}$.
\end{theorem}
The proof of this theorem is fairly straightforward -- just expand $a_n (x - r_1) \dots (x - r_n)$ and it sort of just appears combinatorially.

Later on, towards the end of the 17th century, it's clear that Newton knew how to express many different symmetric sums of roots of a polynomial in terms of its roots. His results were many specific cases of this (important!) theorem:
\begin{definition}
	A polynomial in $n$ variables $P(x_1, \dots, x_n)$ is \textbf{symmetric} if interchanging any two $x_i$ and $x_j$ does not change the polynomial.
\end{definition}
\begin{theorem}[Fundamental Theorem on Symmetric Polynomials]
	Every symmetric polynomial (in $n$ variables) can be expressed as a polynomial in the elementary symmetric polynomials (in $n$ variables) $e_k$.
\end{theorem}
This theorem was widely cited/known before the time of Galois, but the first known proof of this statement comes to us from the 19th century. Oops. We'll prove it here too:
\begin{proof}
	We proceed by induction on $n$, the number of variables. The theorem is sort of silly for $n = 1$, so that'll be our base case. (With one variable, the only symmetric polynomial you can really have is just a polynomial, which is clearly a polynomial in that one variable.)

	Our inductive hypothesis will be that the every symmetric polynomial in $n-1$ variables can be expressed as a polynomial in the elementary symmetric polynomials in $n-1$ variables. Now, let's say we have a symmetric polynomial in $n$ variables, $F(x_1, x_2, \dots, x_n)$. Let's split $F$ into powers of $x_n$, and suppose that the highest degree of $x_n$ that appears is $m$, so $F$ can be decomposed:
	\[
		F(x_1, x_2, \dots, x_n) = F_0 + F_1x_n + F_2 x_n^2 + \dots F_m x_n^m
		.\]
	where now each of the $F_k$s for $0 \leq k \leq m$ have to be symmetric polynomials in $x_1, \dots, x_{n-1}$. This must be true as if some $F_i$ were not symmetric in the $x_1, \dots, x_{n-1}$, then $F$ would not be either. Therefore, we can apply the inductive hypothesis to each of the $F_k$s, so we know that they can all be written as a sum of elementary symmetric polynomials $e_k^{(n-1)}$ in $x_1, \dots, x_{n-1}$. As such, $F_k(x_1, \dots, x_{n-1}) = f_k(e_1^{(n-1)}, \dots, e_{n-1}^{(n-1)})$ for a different polynomial $f_k$ in the $e_k^{(n-1)}$s.

	We're not done yet though, we still need to write $F$ in terms of the elementary symmetric polynomials in $x_1, \dots, x_n$, $e_k^{(n)}$. Let's get one step closer now by relating the $e_k^{(n)}$ to $e_k^{(n-1)}$ in the following way:
	\begin{align*}
		e_1^{(n)}     & = e_1^{(n-1)} + x_n                     \\
		e_2^{(n)}     & = e_2^{(n-1)} + x_n e_1^{(n-1)}         \\
		e_3^{(n)}     & = e_3^{(n-1)} + x_n e_3^{(n-1)}         \\
		\vdots
		e_{n-1}^{(n)} & = e_{n-1}^{(n-1)} + x_n e_{n-2}^{(n-1)} \\
		e_n^{(n)}     & =  x_n e_{n-1}^{(n-1)}                  \\
	\end{align*}
	In particular, we can then write $e_k^{(n-1)}$ as a polynomial in $x_n$ and the $e_k^{(n)}$s (this can easily be seen by induction). This allows us to rewrite each of the $f_k(e_1^{(n-1)}, \dots, e_{n-1}^{(n-1)})$ as polynomials in $e_k^{(n)}$ and $x_n$, which will require us to recollect terms.

	We can reduce this further -- by backsubstituting each of the last equations from the end into the last, we can find the following identity for $e_n^{(n)}$ in terms of $x_n$ and lower $e_k^{(n)}$ for $k < n$:
	\[
		e_n^{(n)} - x_n e_{n-1}^{(n)} + x_n^2 e_{n-2}^{(n)} + \dots + (-1)^n x_n^n = 0
		.\]
	This allows us to reduce the maximal degree until all terms have powers of $x_n$ less than $n$, so we can write
	\[
		F(x_1, \dots, x_n) = h_0(e_1^{(n)}, \dots, e_n^{(n)}) + h_1(e_1^{(n)}, \dots, e_n^{(n)})x_n + \dots + h_{n-1}(e_1^{(n)}, \dots, e_n^{(n)}) x_n^{n-1}
		.\]
	We now want to show that all of the $h_i$s for $i > 0$ are identically 0 to finish the induction.

	Recall again that $F$ is a symmetric polynomial, so swapping $x_n$ with any other $x_i$ should leave the same result. Since each of the $h_k$s are polynomials in elementary symmetric polynomials, swapping $x_n$ and any other $x_i$ would also leave them the same, so this swap would only change the powers of $x_n$. If we do this for every other variable, we get the following system of equations:
	\begin{align*}
		F(x_1, \dots, x_n) & = h_0 + h_1 x_1 + \dots + h_{n-1} x_1^{n-1} \\
		F(x_1, \dots, x_n) & = h_0 + h_1 x_2 + \dots + h_{n-1} x_2^{n-1} \\
		\vdots
		F(x_1, \dots, x_n) & = h_0 + h_1 x_n + \dots + h_{n-1} x_n^{n-1} \\
	\end{align*}
	We can view this as a matrix equation:
	\[
		\bmat{F(x_1, \dots, x_n) \\ F(x_1, \dots, x_n) \\ \vdots \\ F(x_1, \dots, x_n)} = \bmat{ 1 & x_1 & \dots & x_1^{n-1} \\
		1 & x_2 & \dots & x_2^{n-1} \\
		\vdots & \vdots & \ddots & \vdots \\
		1 & x_n & \dots & x_n^{n-1}}
		\bmat{h_0 \\ h_1 \\ \vdots \\ h_{n-1}}
		.\]
	Let the matrix in this equation be $V(x_1, \dots, x_n)$. Note that $V \vect x = \tbmat{F(x_1, \dots, x_n) \\ F(x_1, \dots, x_n) \\ \vdots \\ F(x_1, \dots, x_n)}$ has another solution, namely $\vect x = \tbmat{F(x_1, \dots, x_n) \\ 0 \\ \vdots \\ 0}$. However, note that $V$ is invertible in the general case -- we can show this by showing $\det V \neq 0$ by induction on the number of variables. (In particular, the determinant of $V$ is called the \textbf{Vandermonde determinant}, and it's zero iff two of the $x_i$s are equal.) Therefore, the system has a unique solution and therefore all of the $h_i$s for $i > 0$ must be identically 0.

	(Another way to finish this off -- consider the polynomial $H(y) = -F(x_1, \dots, x_n) + h_0 + h_1 y + \dots + h_{n-1} y^{n-1}$, which has $n$ roots at $x_1, x_2, \dots, x_n$, so it must be divisible by $\prod_{j=1}^n (y - x_n)$. Looking at the degree, however, this is only possible if $H(y) = 0$, which implies the same result).

	Therefore, $F(x_1, \dots, x_n) = h_0(e_1^{(n)}, \dots, e_n^{(n)})$, so the symmetric polynomial $F$ is expressible as a polynomial in the elementary symmetric polynomials in $n$ variables. By induction, then, the statement holds for all $n \geq 1$, as desired.
\end{proof}

\begin{remark}
	\textit{Newton's sums} are a particular case of this theorem, i.e. when $F(x_1, \dots, x_n) = x_1^k + \dots + x_n^k$. Call this polynomial the $k$th power sum of the $x_i$s, $P_k$. The case of the fundamental theorem on symmetric polynomials that is attributed to Newton is a formula for $P_k$ in terms of the elementary symmetric polynomials:
	\[
		P_k = P_{k-1} e_1 - P_{k-2} e_2 + \dots + (-1)^{k} P_1 e_{k-1} + (-1)^{k+1} k e_k
		.\]
	This is useful for finding the sum of the $k$th powers of the roots of a polynomial recursively.
\end{remark}

As a corollary, we now get that \textbf{any symmetric polynomial in the roots of a polynomial is expressible in terms of the coefficients of that polynomial}. This now just follows from combining Vieta's Formulas and the fundamental theorem on symmetric polynomials. This will turn out to be a good motivator for parts of Galois theory down the line when we introduce group actions.


\section{Lagrange Resolvents}
Let's introduce a new way to go about solving polynomial equations (due to Vandermonde and Lagrange) in the late 18th century, now involving the roots of unity. We'll build these up for polynomials of varying degree, starting with the following example for quadratics and working our way upwards. 

\subsection{Quadratics}
Suppose we have a quadratic with roots $x$ and $y$. Then we claim that the expression $\frac 12 ( (x + y) \pm  \sqrt{ (x-y)^2 })$ takes on both of the values of the roots. If we take the positive branch, then $\frac 12( x + y + x - y ) = x$, and on the negative branch, $\frac 12( x + y - (x - y)) = y$. Therefore, in theory, all we would need to do is just evaluate $x + y$ (using Vieta) and $(x - y)^2$ (some other way) and we'd have solved the quadratic. Indeed, we can get $(x-y)^2$ from $(x+y)^2 - 4xy$, which returns us the familiar quadratic formula when we write these expressions in the coefficients of the quadratic. 

\subsection{Cubics}
Okay, that was a little silly, but we can actually do this for higher-degree polynomials too, we just have to use different roots of unity. For a cubic, if $\omega = e^{\frac{2\pi i}3}$, then we should consider the quantity
\[ \frac 13 [(x + y + z) + ((x + \omega y + \omega^2 z)^3)^{\frac 13} + ((x + \omega^2 y  + \omega z)^3)^{\frac 13}].\]
Here we get nine different values upon choosing different branches for the evaluation of the cube roots, only three of which actually produce solutions (in particular, these three are taking no additional factors of $\omega$ on either term, and then putting $\omega$ on one and $\omega^2$ on the other, in either order).

Is this even possible to evaluate? Technically, yes -- if we let $u = (x + \omega y + \omega^2 z)^3$ and $v = (x + \omega^2 y + \omega z)^3$, then $u+v$ and $uv$ are symmetric in $x, y, z$, which means that we can use the results from the previous section to evaluate them both. Note that if we swap $y$ and $z$, both quantities are invariant, and also cyclic permutations $(x y z)$ also leave both invariant, and these generate all permutations of $x, y, z$. As such, it's theoretically possible, but still maybe not great because we still have the issue of getting 9 solutions, but only a third are actually the ones we want. Can we do better?

This first method is due to Vandermonde in 1770 (same guy whose name appears in the determinant in the previous section, and in Vandermonde's identity from combinatorics, but that's maybe it[?]), but Lagrange a little later towards the end of the 18th century (c. 1790) developed a better way using a similar idea.

For a cubic with roots $x, y, z$, consider instead the quantity $t = x + \omega y + \omega^2 z$, where $x, y, z$ are the roots in some order. (Assume here the roots are all distinct -- we'll see how to deal with polynomials where they are not.) As such, $t$ will take on six values, $t_1, t_2, \dots t_6$. Take these values to be the roots of a sixth-degree equation: 
\[
    L(x) = (x - t_1) (x - t_2) (x - t_3) (x - t_4) (x - t_5) (x - t_6)
\]
This polynomial $L(x)$ is called the \textit{Lagrange resolvent}. While it is a sixth-degree polynomial, note that this is actually a quadratic in $x^3$, as for some $t_1$, $\omega t_1$ and $\omega^2 t_1$ are two of the other $t_i$s, so those three terms can be collected as $x^3 - t_1^3$. The other three factors can similarly be collected as $x^3 - t_4^3$ for some suitable labeling. Either way, we can solve for $t_1$ and $t_4$ now, which will be Vandermonde's $x + \omega y + \omega^2 z$ and $x + \omega^2 y + \omega z$, and the coefficients are precisely the above $u + v$ and $uv$. The only problem now is to identify which of the six solutions of the resolvent to use to generate the solutions. 

Lagrange deals with this by stating for any solution of the resolvent, WLOG let $t = x + \omega y + \omega^2 z$. Then, note that $w = (x + \omega y + \omega^2 z)(x + \omega^2 y + \omega z)$ is symmetric in $x, y, z$, as we just see by expansion (or by noting that if $uv$ is symmetric in $x, y, z$, so must its cube root): 
\[
    w = x^2 + \omega^2 xy + \omega xz + \omega xy + y^2 + \omega^2 yz + \omega^2 xz + \omega yz + z^2 = x^2 + y^2 + z^2 - (xy + yz + zx) 
\]
Then, we can just compute $w$ explicitly and then take $\frac w t$ as the corresponding other solution of the resolvent that is needed to match $t$, so we're all good. 

Note that this method relies more on the properties of symmetric polynomials and less of a guess/ansatz, but requires us to solve a higher degree equation. In particular, we can do the same thing with quartics and higher degree polynomials as well. 

\subsection{Quartics}
Quartics turn out to be easier with both methods. For instance, when employing Vandermonde's method for a quartic with roots $w, x, y, z$, it might seem like we might want to use the following quantity to start: 
\[
    \frac 14 [(w + x + y + z) + \sqrt[4]{(w + ix - y - iz)^4} + \sqrt[4]{(w - x + y - z)^4} + \sqrt[4]{(w - ix - y + iz)^4}]
\]
but we actually only need to consider the third term $t = w - x + y - z$ when employing Lagrange's method. Taking into consideration all 24 orderings of these 4 roots, we actually only get 6 possible values for $t$ depending on the ordering ($\pm (w - x + y - z), \pm (w + x - y - z), \pm (w - x - y + z)$). If we call each of these terms $t_1, t_2, t_3$, respectively, then the resolvent is a 24th-degree polynomial, but it can be greatly reduced: 
\[
    L(x) = (x + t_1)^4 (x - t_1)^4 (x + t_2)^4 (x - t_2)^4 (x + t_3)^4 (x - t_3)^4 = (x^2 - t_1^2)^4 (x^2 - t_2^2)^4 (x^2 - t_3^2)^4 
\]  
Hmm, so let's consider instead $M(x) = (x^2 - t_1^2) (x^2 - t_2^2)(x^2 - t_3^2)$, which is a cubic in $x^2$, and whose coefficients are going to be symmetric in $t_1, t_2, t_3$, so we will get symmetry in $w, x, y, z$ as well. This cubic can be solved with the method of Lagrange resolvents or with Cardano's method, so we'll eventually be able to extract $t_1, t_2,$ and $t_3$. 

Like with the cubic, we won't know just from the extraction of the roots of the resolvent ($M(x)$ or otherwise) how to extract the roots. As above, note that the product $p = t_1 t_2 t_3$ is evaluatable as it's symmetric in $w, x, y, z$. Moreover, if we let $s = w + x + y + z$, taking the following sums gives us each of the roots: 
\begin{align*}
    w &= \frac 14 (s + t_1 + t_2 + t_3) \\ 
    x &= \frac 14 (s - t_1 + t_2 - t_3) \\ 
    y &= \frac 14 (s + t_1 - t_2 - t_3) \\ 
    z &= \frac 14 (s - t_1 - t_2 + t_3) \\ 
\end{align*}
Picking any two $t_1$ and $t_2$ that aren't negatives of each other, then, and taking $t_3 = \frac p {t_1 t_2}$ will give us the correct sign for $t_3$. We can then play the signs on the $t_1$ and $t_2$ we choose to get all of the solutions we want. 

\subsection{Quintics and Beyond}
What about doing this for quintics or higher-order polynomials? We simply just have to us higher-order roots of unity -- for instance, for a quintic, a $\zeta = e^{\frac{2\pi}5 i}$ is needed in the term $t$ for the Lagrange resolvent. If our roots are $v, w, x, y, z$, then for the Lagrange method, we want $t = v + \zeta w + \zeta^2 x + \zeta^3 y + \zeta^4 z$, which results in 120 distinct terms, and a degree-120 polynomial as our resolvent. We can simplify by noting that if $t$ is a root of the resolvent, so are $\zeta t$, $\zeta^2 t$, $\zeta^3 t$, and $\zeta^4 t$, so collecting these all together we know we can refactor this as a factor of $x^5 - t^5$ for each distinct ordering of the roots under cyclic permutations. This allows us to write the resolvent as a degree-24 polynomial in $x^5$\dots if only we could know how to proceed by solving degree-24 polynomial equations! So we appear to be kind of stuck here...

The situation is not much better for larger $n$ -- as $n$ grows, the degree of the resolvent grows like $n!$, and it becomes essentially impossible to solve these resolvents for their roots. So we appear to be stuck again in our quest to solve polynomial equations, but we at least have a general procedure for any polynomial! 


\section{Aside -- Gauss and Roots of Unity}
We've been discussing the roots of unity when we introduced Lagrange resolvents, so we'll focus on them a little more now. Recall that $n$th roots of unity are of the form $\cos\left(\frac{2\pi k} n \right) + i \sin \left( \frac{2\pi k} n \right) = e^{\frac{2\pi k}n i}$ for $0 \leq k < n$. In order to manipulate these properly, we'd like to be able to compute these algebraically in terms of roots and standard algebraic operations. Define a \textit{primitive} $n$th root of unity to be a root of unity $e^{\frac{2\pi k}n i}$ such that for any $m < n$, $(e^{\frac{2\pi k}n i})^m \neq 1$. This is equivalent to saying that we want $\gcd(k, n) = 1$, as otherwise if $\gcd(k, n) = d > 1$, then $(e^{\frac{2\pi k}n i})^{\frac n d} = 1$ (and the converse is also true). 

Now, to reduce our work down, note that if $\gcd(a, b) = 1$ and $n = ab = \lcm(a, b)$, a primitive $n$th root of unity can be obtained by multiplying together a primitive $a$th root of unity and a primitive $b$th root of unity. This is some standard number theory fare -- note that $\omega$ and $\zeta$ are such roots of unity, then $\omega = e^{\frac{2\pi k}a i}$ and $\zeta = e^{\frac{2\pi l}b i}$, so $\omega \zeta = e^{2\pi i (\frac {kb + la}{ab})}$ with $\gcd(k, a) = 1$ and $\gcd(l, b) = 1$. Note then that $\gcd(kb + la, ab) = 1$ also, so this must be an $ab$th root of unity as well. 

As such, any $n$ can be broken into its prime powers, so it now suffices to find primitive $p^k$th roots of unity. However, a primitive $p^k$th root of unity is merely a $p^{k-1}$st root of a $p$th root of unity by the equivalent formulation of primitive roots of unity, so it suffices really to construct $p$th roots of unity algebraically. So let's do it!  

\subsection{$p$th Roots of Unity}
This will be initially a little bit of number theory to start. First, we need the existence of a primitive root mod $p$: 
\begin{lemma}[Existence of Primitive Roots mod $p$]
For every prime $p$, there is a positive integer $g$ such that every $0 < m < p-1$, $g^m \not\equiv 1 \mod p$, i.e. the smallest positive integer $k$ such that $g^k \equiv 1 \mod p$ is $p-1$. 
\end{lemma}
I don't really remember how to prove this or what the proof looks like, so I'll put a proof here so I'll remember at least one for the future: 
\begin{proof}
haha haven't done it yet
\end{proof}

\section{Galois Resolvents}
In general, Lagrange's idea for solving a degree-$n$ polynomial equation is to concoct some sort of \textit{resolvent}. We saw some examples of resolvents that worked for cubics and quartics, but here are some properties that a general resolvent should have: 
\begin{itemize}
    \item It needs to be a polynomial in the elementary symmetric sums of the roots/coefficients of the polynomial and the $n$th roots of unity with rational coefficients (so we know this polynomial is evaluatable).
    \item We also want the resolvent to help us solve for the roots of the original polynomial.
\end{itemize}  

\section{Galois Groups}

\section{Groups of Solvable Equations}

\section{Splitting Fields}

\section{The Fundamental Theorem of Galois Theory}

\end{document}