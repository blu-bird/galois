\documentclass[12pt]{scrartcl}
\usepackage{blubird}
% has options [nodate, nosans, nofancy, nocolor, code]

% box setup - should be moved to somewhere nicer
\mdfsetup{
	roundcorner = 2pt,
	linewidth = 1pt,
	innertopmargin = 0.5em,
	innerbottommargin = 1em,
	frametitlefont = \bfseries,
}

% TITLE
\title{Galois Theory -- Historical Perspective}
\author{Bryan Lu}
\date{May-June 2022} % do not use if [nodate] option enabled

\begin{document}
\maketitle

\setcounter{section}{-1}
\section{Foreword}
These are some notes on Galois theory following H. M. Edwards' \textit{Galois Theory}. Galois theory is an overlooked subject in Cornell's algebra sequence -- it's only given a passing mention in MATH 4340, the honors abstract algebra course, and it's kind of a shame it's treated this way. At its heart is a nice connection between fields and groups and I feel like it's a big part of why one would want to study fields in the first place. I also think Galois theory is important in leveling up an understanding of number theory as well as dealing with fields and number systems in general beyond just the integers, so I think it's useful in algebraic NT...? I really don't know much more in this regard so I should stop talking lol but those are my thoughts on why it's important, at least.

I also think that there's a lot of historical mystique around the field as well -- a lot of people know the story of how Evariste Galois was shot and killed in a duel at age 21, and the night before he died he feverishly wrote down the groundwork for this really amazing theory. What a story, right? I kinda like Edwards' book because the smoke and mirrors are cleared here -- you get the benefit of the historical context in which Galois was working, and also understand the angle at which he was coming from when he wrote all that stuff without losing the modern interpretation of the results. I don't think this is a very standard treatment of the subject at all, but I think it's fun nonetheless and will do some mixing in with Dummit and Foote as needed.

Some prerequisites -- while you don't technically need to have any background in algebra to understand the book, I will assume some familiarity with group theory and ring theory (i.e. ideals, PIDs, UFDs, etc.) and linear algebra at the level of MATH 4330-4340 in these notes. These notes are a distillation of the important developments in the field's history and while the book does go out of its way to also explain groups and rings and stuff (or pull teeth to try and get the tools it needs), I won't. There's also the whole story about solving the cubic/quartic with Cardano and Tartaglia and Ferrari and del Ferro which is commonly used as historical setup for Abel-Ruffini, a theorem often discussed in a treatment Galois theory -- if you're interested in the story, I won't rehash it here as so many people do it, but if you don't know about it you should find out! Veritasium has a really nice popular depiction of it and he does a much better job than I ever could so I'll assume you'll get the story from elsewhere and I won't have to talk about it.

\section{History of Solving Polynomial Equations}
A brief history of solving polynomial equations (of degree higher than 1):
\begin{itemize}
	\item The Babylonians knew how to solve quadratics! This is akin to Po-Shen Loh's method of solving quadratics today -- conceiving it instead as a puzzle in which you have to figure out two numbers (the roots of a quadratic) given only their sum and product.
	\item Not a lot of progress on cubics and quartics until the aforementioned developments by del Ferro/Cardano/Tartaglia in the 16th century -- there's a surprising amount of drama and tea here. The main ideas:
	      \begin{itemize}
		      \item Depressing the cubic -- take cubics of the form $x^3 + ax^2 + bx + c$ and introduce the change of variables $y = x - \frac a 3$, which eliminates the quadratic term. Leaves you with an equation of the form $y^3 + py + q$.
		      \item Guess that $y = m - n$. Substituting, you can see that a good heuristic for the solution is that $3mn = p$, $m^3 - n^3 + q = 0$. Note that $27m^6 - 27 m^3 n^3 + 27 m^3 q = 0$ as well, but $27 m^3 n^3 = p^3$, so you actually have a quadratic in $m^3$ now, allowing you to solve for $m$ and $n$ now, and then unwinding to get $y$ and then $x$.
	      \end{itemize}
	\item Quartics would fall very shortly after due to Ferrari. This is more undisputed I think? Again, just the main ideas:
	      \begin{itemize}
		      \item Depress the quartic now -- introduce the change of variables $y = x - \frac a 4$ for the quartic $x^4 + ax^3 + bx^2 + cx + d$, which leaves a new quartic $y^4 + py^2 + qy + r$.
		      \item Introduce a new variable $z$, and consider $(y^2 + z)^2 = y^4 + 2 y^2z + z^2 = (-p + 2z)y^2 - qy - r + z^2$. Heuristically, we want to be able to take the square root of the RHS. This means that this quadratic in $y$ actually has to have a single root, so this happens when $q^2 - 4(-p + 2z)(z^2 - r) = 0$. This gives a cubic in $z$ that can be solved!
		      \item Solve for $z$ using the procedure for a cubic. If $z$ is now chosen appropriately, we get that $(y^2 + z) = \pm \sqrt{-p + 2z} \left(y - \frac q{2(-p + 2z)} \right)$, which gives a quadratic that can now be solved.
	      \end{itemize}
	\item Abel shows in the 19th century (c. 1820) that it's impossible to solve qunitics or higher in general form in the same way as above. (This proof is not based in Galois theory, but uses some of the techniques mentioned in future sections.)

	      Note that this is about a 300-year gap without seemingly much progress on this problem! What were people up to in that time?
\end{itemize}

\subsection{Fundamental Theorem on Symmetric Polynomials, Vieta's Formulas, Newton's Sums}
One of the most important developments in algebra (for us) in the time between the time of the Great Italian War of Cubics and Quartics and Abel's big contribution in the early 1800s is a development in the theory of symmetric polynomials.

First, I feel like I have to mention Francois Viete here, along with Vieta's formulas -- discovered in the late 16th century. I do need to provide a definition first --
\begin{definition}
	The \textbf{$k$th elementary symmetric polynomial} on $n$ variables $x_1, x_2, \dots, x_n$, $e_k(x_1, x_2, \dots, x_n)$ is the sum of all $\binom n k$ terms $x_{i_1} x_{i_2} \dots x_{i_k}$ ($i_1 < i_2 < \dots < i_k$) where all of the $i_j \in [n]$.
\end{definition}
Viete figured out how to express elementary symmetric polynomials in the roots of a polynomial in terms of the coefficients of that polynomial:
\begin{theorem}[Vieta's Formulas]
	If a polynomial $a_n x^n + \dots + a_1 x + a_0$ has roots $r_1, r_2, \dots, r_n$, then the elementary symmetric polynomials are expressible in terms of the roots. In particular, $e_{k}(r_1, r_2, \dots, r_n) = (-1)^{k} \frac{a_{n-k}}{a_n}$.
\end{theorem}
The proof of this theorem is fairly straightforward -- just expand $a_n (x - r_1) \dots (x - r_n)$ and it sort of just appears combinatorially.

Later on, towards the end of the 17th century, it's clear that Newton knew how to express many different symmetric sums of roots of a polynomial in terms of its roots. His results were many specific cases of this (important!) theorem:
\begin{definition}
	A polynomial in $n$ variables $P(x_1, \dots, x_n)$ is \textbf{symmetric} if interchanging any two $x_i$ and $x_j$ does not change the polynomial.
\end{definition}
\begin{theorem}[Fundamental Theorem on Symmetric Polynomials]
	Every symmetric polynomial (in $n$ variables) can be expressed as a polynomial in the elementary symmetric polynomials (in $n$ variables) $e_k$.
\end{theorem}
This theorem was widely cited/known before the time of Galois, but the first known proof of this statement comes to us from the 19th century. Oops. We'll prove it here too:
\begin{proof}
	We proceed by induction on $n$, the number of variables. The theorem is sort of silly for $n = 1$, so that'll be our base case. (With one variable, the only symmetric polynomial you can really have is just a polynomial, which is clearly a polynomial in that one variable.)

	Our inductive hypothesis will be that the every symmetric polynomial in $n-1$ variables can be expressed as a polynomial in the elementary symmetric polynomials in $n-1$ variables. Now, let's say we have a symmetric polynomial in $n$ variables, $F(x_1, x_2, \dots, x_n)$. Let's split $F$ into powers of $x_n$, and suppose that the highest degree of $x_n$ that appears is $m$, so $F$ can be decomposed:
	\[
		F(x_1, x_2, \dots, x_n) = F_0 + F_1x_n + F_2 x_n^2 + \dots F_m x_n^m
		.\]
	where now each of the $F_k$s for $0 \leq k \leq m$ have to be symmetric polynomials in $x_1, \dots, x_{n-1}$. This must be true as if some $F_i$ were not symmetric in the $x_1, \dots, x_{n-1}$, then $F$ would not be either. Therefore, we can apply the inductive hypothesis to each of the $F_k$s, so we know that they can all be written as a sum of elementary symmetric polynomials $e_k^{(n-1)}$ in $x_1, \dots, x_{n-1}$. As such, $F_k(x_1, \dots, x_{n-1}) = f_k(e_1^{(n-1)}, \dots, e_{n-1}^{(n-1)})$ for a different polynomial $f_k$ in the $e_k^{(n-1)}$s.

	We're not done yet though, we still need to write $F$ in terms of the elementary symmetric polynomials in $x_1, \dots, x_n$, $e_k^{(n)}$. Let's get one step closer now by relating the $e_k^{(n)}$ to $e_k^{(n-1)}$ in the following way:
	\begin{align*}
		e_1^{(n)}     & = e_1^{(n-1)} + x_n                     \\
		e_2^{(n)}     & = e_2^{(n-1)} + x_n e_1^{(n-1)}         \\
		e_3^{(n)}     & = e_3^{(n-1)} + x_n e_3^{(n-1)}         \\
		\vdots
		e_{n-1}^{(n)} & = e_{n-1}^{(n-1)} + x_n e_{n-2}^{(n-1)} \\
		e_n^{(n)}     & =  x_n e_{n-1}^{(n-1)}                  \\
	\end{align*}
	In particular, we can then write $e_k^{(n-1)}$ as a polynomial in $x_n$ and the $e_k^{(n)}$s (this can easily be seen by induction). This allows us to rewrite each of the $f_k(e_1^{(n-1)}, \dots, e_{n-1}^{(n-1)})$ as polynomials in $e_k^{(n)}$ and $x_n$, which will require us to recollect terms.

	We can reduce this further -- by backsubstituting each of the last equations from the end into the last, we can find the following identity for $e_n^{(n)}$ in terms of $x_n$ and lower $e_k^{(n)}$ for $k < n$:
	\[
		e_n^{(n)} - x_n e_{n-1}^{(n)} + x_n^2 e_{n-2}^{(n)} + \dots + (-1)^n x_n^n = 0
		.\]
	This allows us to reduce the maximal degree until all terms have powers of $x_n$ less than $n$, so we can write
	\[
		F(x_1, \dots, x_n) = h_0(e_1^{(n)}, \dots, e_n^{(n)}) + h_1(e_1^{(n)}, \dots, e_n^{(n)})x_n + \dots + h_{n-1}(e_1^{(n)}, \dots, e_n^{(n)}) x_n^{n-1}
		.\]
	We now want to show that all of the $h_i$s for $i > 0$ are identically 0 to finish the induction.

	Recall again that $F$ is a symmetric polynomial, so swapping $x_n$ with any other $x_i$ should leave the same result. Since each of the $h_k$s are polynomials in elementary symmetric polynomials, swapping $x_n$ and any other $x_i$ would also leave them the same, so this swap would only change the powers of $x_n$. If we do this for every other variable, we get the following system of equations:
	\begin{align*}
		F(x_1, \dots, x_n) & = h_0 + h_1 x_1 + \dots + h_{n-1} x_1^{n-1} \\
		F(x_1, \dots, x_n) & = h_0 + h_1 x_2 + \dots + h_{n-1} x_2^{n-1} \\
		\vdots
		F(x_1, \dots, x_n) & = h_0 + h_1 x_n + \dots + h_{n-1} x_n^{n-1} \\
	\end{align*}
	We can view this as a matrix equation:
	\[
		\bmat{F(x_1, \dots, x_n) \\ F(x_1, \dots, x_n) \\ \vdots \\ F(x_1, \dots, x_n)} = \bmat{ 1 & x_1 & \dots & x_1^{n-1} \\
		1 & x_2 & \dots & x_2^{n-1} \\
		\vdots & \vdots & \ddots & \vdots \\
		1 & x_n & \dots & x_n^{n-1}}
		\bmat{h_0 \\ h_1 \\ \vdots \\ h_{n-1}}
		.\]
	Let the matrix in this equation be $V(x_1, \dots, x_n)$. Note that $V \vect x = \tbmat{F(x_1, \dots, x_n) \\ F(x_1, \dots, x_n) \\ \vdots \\ F(x_1, \dots, x_n)}$ has another solution, namely $\vect x = \tbmat{F(x_1, \dots, x_n) \\ 0 \\ \vdots \\ 0}$. However, note that $V$ is invertible in the general case -- we can show this by showing $\det V \neq 0$ by induction on the number of variables. (In particular, the determinant of $V$ is called the \textbf{Vandermonde determinant}, and it's zero iff two of the $x_i$s are equal.) Therefore, the system has a unique solution and therefore all of the $h_i$s for $i > 0$ must be identically 0.

	(Another way to finish this off -- consider the polynomial $H(y) = -F(x_1, \dots, x_n) + h_0 + h_1 y + \dots + h_{n-1} y^{n-1}$, which has $n$ roots at $x_1, x_2, \dots, x_n$, so it must be divisible by $\prod_{j=1}^n (y - x_n)$. Looking at the degree, however, this is only possible if $H(y) = 0$, which implies the same result).

	Therefore, $F(x_1, \dots, x_n) = h_0(e_1^{(n)}, \dots, e_n^{(n)})$, so the symmetric polynomial $F$ is expressible as a polynomial in the elementary symmetric polynomials in $n$ variables. By induction, then, the statement holds for all $n \geq 1$, as desired.
\end{proof}

\begin{remark}
	\textit{Newton's sums} are a particular case of this theorem, i.e. when $F(x_1, \dots, x_n) = x_1^k + \dots + x_n^k$. Call this polynomial the $k$th power sum of the $x_i$s, $P_k$. The case of the fundamental theorem on symmetric polynomials that is attributed to Newton is a formula for $P_k$ in terms of the elementary symmetric polynomials:
	\[
		P_k = P_{k-1} e_1 - P_{k-2} e_2 + \dots + (-1)^{k} P_1 e_{k-1} + (-1)^{k+1} k e_k
		.\]
	This is useful for finding the sum of the $k$th powers of the roots of a polynomial recursively.
\end{remark}

As a corollary, we now get that \textbf{any symmetric polynomial in the roots of a polynomial is expressible in terms of the coefficients of that polynomial}. This now just follows from combining Vieta's Formulas and the fundamental theorem on symmetric polynomials. This will turn out to be a good motivator for parts of Galois theory down the line when we introduce group actions.


\section{Lagrange Resolvents}
Let's introduce a new way to go about solving polynomial equations (due to Vandermonde and Lagrange) in the late 18th century, now involving the roots of unity. We'll build these up for polynomials of varying degree, starting with the following example for quadratics and working our way upwards. 

\subsection{Quadratics}
Suppose we have a quadratic with roots $x$ and $y$. Then we claim that the expression $\frac 12 ( (x + y) \pm  \sqrt{ (x-y)^2 })$ takes on both of the values of the roots. If we take the positive branch, then $\frac 12( x + y + x - y ) = x$, and on the negative branch, $\frac 12( x + y - (x - y)) = y$. Therefore, in theory, all we would need to do is just evaluate $x + y$ (using Vieta) and $(x - y)^2$ (some other way) and we'd have solved the quadratic. Indeed, we can get $(x-y)^2$ from $(x+y)^2 - 4xy$, which returns us the familiar quadratic formula when we write these expressions in the coefficients of the quadratic. 

\subsection{Cubics}
Okay, that was a little silly, but we can actually do this for higher-degree polynomials too, we just have to use different roots of unity. For a cubic, if $\omega = e^{\frac{2\pi i}3}$, then we should consider the quantity
\[ \frac 13 [(x + y + z) + ((x + \omega y + \omega^2 z)^3)^{\frac 13} + ((x + \omega^2 y  + \omega z)^3)^{\frac 13}].\]
Here we get nine different values upon choosing different branches for the evaluation of the cube roots, only three of which actually produce solutions (in particular, these three are taking no additional factors of $\omega$ on either term, and then putting $\omega$ on one and $\omega^2$ on the other, in either order).

Is this even possible to evaluate? Technically, yes -- if we let $u = (x + \omega y + \omega^2 z)^3$ and $v = (x + \omega^2 y + \omega z)^3$, then $u+v$ and $uv$ are symmetric in $x, y, z$, which means that we can use the results from the previous section to evaluate them both. Note that if we swap $y$ and $z$, both quantities are invariant, and also cyclic permutations $(x y z)$ also leave both invariant, and these generate all permutations of $x, y, z$. As such, it's theoretically possible, but still maybe not great because we still have the issue of getting 9 solutions, but only a third are actually the ones we want. Can we do better?

This first method is due to Vandermonde in 1770 (same guy whose name appears in the determinant in the previous section, and in Vandermonde's identity from combinatorics, but that's maybe it[?]), but Lagrange a little later towards the end of the 18th century (c. 1790) developed a better way using a similar idea.

For a cubic with roots $x, y, z$, consider instead the quantity $t = x + \omega y + \omega^2 z$, where $x, y, z$ are the roots in some order. (Assume here the roots are all distinct -- we'll see how to deal with polynomials where they are not.) As such, $t$ will take on six values, $t_1, t_2, \dots t_6$. Take these values to be the roots of a sixth-degree equation: 
\[
    L(x) = (x - t_1) (x - t_2) (x - t_3) (x - t_4) (x - t_5) (x - t_6)
\]
The expression for $t$ is called the \textit{resolvent}, and the auxiliary polynomial $L(x)$ is called the \textit{resolvent equation}. While it is a sixth-degree polynomial, note that this is actually a quadratic in $x^3$, as for some $t_1$, $\omega t_1$ and $\omega^2 t_1$ are two of the other $t_i$s, so those three terms can be collected as $x^3 - t_1^3$. The other three factors can similarly be collected as $x^3 - t_4^3$ for some suitable labeling. Either way, we can solve for $t_1$ and $t_4$ now, which will be Vandermonde's $x + \omega y + \omega^2 z$ and $x + \omega^2 y + \omega z$, and the coefficients are precisely the above $u + v$ and $uv$. The only problem now is to identify which of the six solutions of the resolvent equation to use to generate the solutions. 

Lagrange deals with this by taking any solution of the resolvent equation, WLOG let $t = x + \omega y + \omega^2 z$. Then, note that $w = (x + \omega y + \omega^2 z)(x + \omega^2 y + \omega z)$ is symmetric in $x, y, z$, as we just see by expansion (or by noting that if $uv$ is symmetric in $x, y, z$, so must its cube root): 
\[
    w = x^2 + \omega^2 xy + \omega xz + \omega xy + y^2 + \omega^2 yz + \omega^2 xz + \omega yz + z^2 = x^2 + y^2 + z^2 - (xy + yz + zx) 
\]
Then, we can just compute $w$ explicitly and then take $\frac w t$ as the corresponding other solution of the resolvent equation that is needed to match $t$, so we're all good. 

Note that this method relies more on the properties of symmetric polynomials and less of a guess/ansatz, but requires us to solve a higher degree equation. In particular, we can do the same thing with quartics and higher degree polynomials as well. 

\subsection{Quartics}
Quartics turn out to be easier with both methods. For instance, when employing Vandermonde's method for a quartic with roots $w, x, y, z$, it might seem like we might want to use the following quantity to start: 
\[
    \frac 14 [(w + x + y + z) + \sqrt[4]{(w + ix - y - iz)^4} + \sqrt[4]{(w - x + y - z)^4} + \sqrt[4]{(w - ix - y + iz)^4}]
\]
but we actually only need to consider the third term $t = w - x + y - z$ when employing Lagrange's method. Taking into consideration all 24 orderings of these 4 roots, we actually only get 6 possible values for $t$ depending on the ordering ($\pm (w - x + y - z), \pm (w + x - y - z), \pm (w - x - y + z)$). If we call each of these terms $t_1, t_2, t_3$, respectively, then the resolvent equation is a 24th-degree polynomial, but it can be greatly reduced: 
\[
    L(x) = (x + t_1)^4 (x - t_1)^4 (x + t_2)^4 (x - t_2)^4 (x + t_3)^4 (x - t_3)^4 = (x^2 - t_1^2)^4 (x^2 - t_2^2)^4 (x^2 - t_3^2)^4 
\]  
Hmm, so let's consider instead $M(x) = (x^2 - t_1^2) (x^2 - t_2^2)(x^2 - t_3^2)$, which is a cubic in $x^2$, and whose coefficients are going to be symmetric in $t_1, t_2, t_3$, so we will get symmetry in $w, x, y, z$ as well. This cubic can be solved with the method of resolvents or with Cardano's method, so we'll eventually be able to extract $t_1, t_2,$ and $t_3$. 

Like with the cubic, we won't know just from the extraction of the roots of the resolvent equation ($M(x)$ or otherwise) how to extract the roots. As above, note that the product $p = t_1 t_2 t_3$ is evaluatable as it's symmetric in $w, x, y, z$. Moreover, if we let $s = w + x + y + z$, taking the following sums gives us each of the roots: 
\begin{align*}
    w &= \frac 14 (s + t_1 + t_2 + t_3) \\ 
    x &= \frac 14 (s - t_1 + t_2 - t_3) \\ 
    y &= \frac 14 (s + t_1 - t_2 - t_3) \\ 
    z &= \frac 14 (s - t_1 - t_2 + t_3) \\ 
\end{align*}
Picking any two $t_1$ and $t_2$ that aren't negatives of each other, then, and taking $t_3 = \frac p {t_1 t_2}$ will give us the correct sign for $t_3$. We can then play the signs on the $t_1$ and $t_2$ we choose to get all of the solutions we want. 

\subsection{Quintics and Beyond}
What about doing this for quintics or higher-order polynomials? We simply just have to us higher-order roots of unity -- for instance, for a quintic, a $\zeta = e^{\frac{2\pi}5 i}$ is needed in the term $t$ for the resolvent. If our roots are $v, w, x, y, z$, then for the Lagrange method, we want $t = v + \zeta w + \zeta^2 x + \zeta^3 y + \zeta^4 z$, which results in 120 distinct terms, and a degree-120 polynomial as our resolvent equation. We can simplify by noting that if $t$ is a root of the resolvent equation, so are $\zeta t$, $\zeta^2 t$, $\zeta^3 t$, and $\zeta^4 t$, so collecting these all together we know we can refactor this as a factor of $x^5 - t^5$ for each distinct ordering of the roots under cyclic permutations. This allows us to write the resolvent equation as a degree-24 polynomial in $x^5$\dots if only we could know how to proceed by solving degree-24 polynomial equations! So we appear to be kind of stuck here...

The situation is not much better for larger $n$ -- as $n$ grows, the degree of the resolvent equation grows like $n!$, and it becomes essentially impossible to solve these equations for their roots. So we appear to be stuck again in our quest to solve polynomial equations, but we at least have a general procedure for any polynomial. The only issue is that when computing the resolvent/resolvent equations, we may have to compute some roots of unity in order to compute its coefficients in general, and doing this in general is not necessarily the easiest thing (for roots of unity with factors of primes greater than 11, for instance). Gauss was able to find a way to write these roots of unity fully in terms of radicals, but we'll revisit this when his construction is under more solid footing. 

As a historical note, the particular form of the expressions $t$ that we've chosen, e.g. $x + \omega y + \omega^z$, where we have roots of unity as coefficients for each supposed root of the original equation, is called a \textit{Lagrange resolvent}. We could theoretically take any general polynomial in the $n$ roots of a polynomial and try to use them as the roots of a more solvable polynomial when permuting the $n$ roots in all possible ways, and so we want to now study resolvents in general. 


\section{Galois Resolvents}
In general, Lagrange's idea for solving a degree-$n$ polynomial equation is to concoct some sort of \textit{resolvent}. We saw some examples of resolvents that worked for cubics and quartics, but here are some properties that Lagrange thought a useful, general resolvent should have: 
\begin{enumerate}[label=\roman*)]
    \item It needs to be a polynomial in the roots/coefficients of the polynomial and the $n$th roots of unity, with rational coefficients otherwise (assuming that we will be able to evaluate $n$th roots of unity. This will allow us to be able to evaluate the coefficients of the resolvent equation. 
    \item We also want to be able to the resolvent to help us solve for the roots of the original polynomial, so we want to be able to rationally express the roots of the original polynomial in terms of the different values of the resolvent. 
    \item Hopefully, we should be able to compute the roots of the resolvent equation.
\end{enumerate}
These resolvents seem to be hard to find for polynomials of degree 5 or higher -- at least by example, as Lagrange's resolvent didn't work. Lagrange worked on trying to characterize resolvents with properties i) and ii), which served as foundational work for Galois' work. 

\subsection{Lagrange's Rational Function Theorem}
Here's Lagrange's theorem on resolvents, which talks about being able to rationally express certain polynomials in the roots of the polynomials in terms of others if they behave the same under permutations. First, let's define an action of permutations $\pi \in S_n$ on polynomials in $n$ variables: 
\begin{definition}
    If $\pi \in S_n$, and $p(x_1, \dots, x_n)$ is a polynomial in $n$ variables, then define the group action of $S_n$ on $p$ by $\pi p = p(x_{\pi(1)}, \dots, x_{\pi(n)})$. 
\end{definition}
\begin{theorem}[Rational Function Theorem]
    Let $p(x_1, \dots, x_n)$ and $q(x_1, \dots, x_n)$ be polynomials (our candidate resolvents), and let $\pi_1 p, \dots, \pi_k p$ be the formally distinct polynomials obtained by applying all $\pi \in S_n$ to $p(x_1, \dots, x_n)$. If $q$ is such that for any $\pi \in S_n$ such that $\pi p = p$ implies $\pi q = q$, then $q(x_1, \dots, x_n)$ can be expressed as a rational function in terms of $p(x_1, \dots, x_n)$ and elementary symmetric polynomials the $n$ variables $x_i$ (i.e. as a quotient of polynomials involving these known quantities).  
\end{theorem}
\begin{remark}
    Suppose we define the resolvent polynomial $L(X)$ similarly to how we did in the last section: 
    \[
        L(X; x_1, \dots, x_n) = (X - \pi_1 p(\vect x)) \dots (X - \pi_k p(\vect x)) = \prod_{i=1}^k (X - \pi_i p(x_1, \dots, x_n)) 
    \]  
    where $\vect x$ is shorthand for $(x_1, \dots, x_n)$. 
    If we plug into $p$ very specific (unknown) roots $r_1, \dots, r_n$ of some $n$th degree polynomial, note that $L(X; r_1, \dots, r_n)$ may not have simple roots -- it might be the case that two of the roots could be numerically coincident, especially if two of the roots are coincident. We'll see how to deal with this later, but this has no bearing on the truth of Lagrange's Rational Function Theorem. Just see that the roots of $L(X)$ will also be expressible in terms of values of $q$, which allows us to interchange resolvents essentially. 
    
    As another note, if we fix the (unknown) roots we're plugging in as $r_1, \dots, r_n$ as the roots of some given polynomial, then note that the coefficients of $L(X)$ will be evaluatable using the coefficients of that polynomial, i.e. the elementary symmetric polynomials in those roots. Note that the polynomial $L(X)$ has roots that are all of the formally distinct values of the resolvent with the roots permuted in all different ways. In particular, switching the labels on any two of the roots won't change the set of roots of $L(X)$ [the $\pi_i p(r_1, \dots, r_n)$s], and $L(X)$ has coefficients that are symmetric polynomials in the $\pi_i p(r_1, \dots, r_n)$s, so the coefficients of $L(x)$ are also symmetric polynomials in the $r_j$s. That means that we can evaluate the coefficients of $L(X)$ by the fundamental theorem of symmetric polynomials, so this construction for the resolvent equation is at least a reasonable quantity that we could always try and compute. 
\end{remark}
Let's prove the Rational Function Theorem with a little linear algebra now: 
\begin{proof}
Define the $r_j$s, $p$, $q$, and $\pi_i p$s and $\pi_i q$s as above/corresponding to the setup in the theorem, and in particular suppose that $\pi_1 p, \dots, \pi_k p$ are all of the formally distinct polynomials obtained by permuting the variables in $p$, where the $\pi_j$s are all elements of $S_n$. Let's now also take the corresponding $\pi_1 q$, \dots, $\pi_k q$. First, we need to show that this is also an exhaustive list of all possible $\pi q$ for $\pi \in S_n$. 

First, consider the stabilizer $S_p$ of the action of $S_n$ on $p$, i.e. the set of all $\pi \in S_n$ such that $\pi p = p$. $S_p$ is a subgroup of $S_n$, and as such every element $\tau \in S_n$ is in a left coset of $S_p$. Since each of the $\pi_i p$s are distinct, note that each $\pi_i$ must be in a distinct left coset of $S_p$. (Note if some $\pi_i$ and $\pi_j$ are in the same left coset of $S_p$, then $\pi_i$ and $\pi_j$ would have to produce the same action on $p$.) Then, each $\pi_i$ is a representative for a left coset of $S_p$, so then every $\tau \in S_n$ is equal to some $\pi_i \sigma$, for $\sigma \in S_p$. As such, $\tau q = \pi_i \sigma q$, but then by hypothesis, if $\sigma$ is in the stabilizer of the action of $S_n$ on $p$, then it's also in the stabilizer of the action of $S_n$ on $q$, so $\tau q = \pi_i q$. Therefore, the set of all $\pi_i q$ is in fact exhaustive. 

At this point, for the sake of brevity, let $p_i = \pi_i p$ and $q_i = \pi_i q$. Let's now consider the following expressions: 
\begin{align*}
    & q_1 + q_2 + \dots + q_k \\ 
    & p_1 q_1 + p_2 q_2 + \dots + p_k q_k \\ 
    & p_1^2 q_1 + p_2^2 q_2 + \dots + p_k^2 q_k \\ 
    & \qquad \qquad \quad \vdots \\
    & p_1^{k-1} q_1 + p_2^{k-1} q_2 + \dots + p_k^{k-1} q_k 
\end{align*}
Note that each of these is a symmetric polynomial in the $x_i$s. Note that permuting the $x_i$s just swaps the $p_i$s with each other, and swaps the $q_i$s in the same way, so these are all symmetric in the $x_i$s, and are therefore expressible as a polynomial in the elementary symmetric polynomials in the $x_i$s. As such, let $h_j(e_1, \dots, e_n) = \sum_{i=1}^k p_i^j q_i$ for $0 \leq j < k$, and note that each of these $h_j$s are known. 
Let's now view this system of equalities in the following way: 
\[
    \bmat{1 & 1 & \dots & 1 \\
          p_1 & p_2 & \dots & p_k \\ 
          \vdots & \vdots & \ddots & \vdots \\ 
          p_1^{k-1} & p_2^{k-1} & \dots & p_k^{k-1}}
    \bmat{q_1 \\ q_2 \\ \vdots \\ q_k } = \bmat{h_0(e_1, \dots, e_n) \\ h_1 (e_1, \dots, e_n) \\ \vdots \\  h_{k-1}(e_1, \dots, e_n)} 
\]
One of the $p_i$s must be $p$ exactly, WLOG suppose $p_1 = p$, so $q_1 = q$ also. We will now attempt to solve for $q_1$ with Cramer's rule now. Recall that Cramer's rule says that
\[
    q_1 = \frac{\det \bmat{h_0 & 1 & \dots & 1 \\
          h_1 & p_2 & \dots & p_k \\ 
          \vdots & \vdots & \ddots & \vdots \\ 
          h_{k-1} & p_2^{k-1} & \dots & p_k^{k-1}}}{\det \bmat{1 & 1 & \dots & 1 \\
          p_1 & p_2 & \dots & p_k \\ 
          \vdots & \vdots & \ddots & \vdots \\ 
          p_1^{k-1} & p_2^{k-1} & \dots & p_k^{k-1}}}
\]
Recall that this denominator is the Vandermonde determinant of the $p_i$s, and we know it's not zero by construction as it's equal to $\det V(p_1 \dots, p_k) = \prod_{1 \leq i < j \leq k} (p_j - p_i)$. We'll do something a little contrived though -- 
\[
    q_1 = \frac{\det V(p_1, \dots, p_k) \cdot \det \bmat{h_0 & 1 & \dots & 1 \\
          h_1 & p_2 & \dots & p_k \\ 
          \vdots & \vdots & \ddots & \vdots \\ 
          h_{k-1} & p_2^{k-1} & \dots & p_k^{k-1}} }{\det V(p_1, \dots, p_k)^2}
\]
Regardless of the squaring, the denominator is symmetric in the $p_i$s, which are collectively symmetric in the $x_i$s, so this denominator is certainly expressible in terms of elementary symmetric polynomials in the $x_i$s. It suffices to treat the numerator. 

We now claim that the polynomial in the numerator is symmetric in the $p_2, \dots, p_k$. If we swap some $p_i$ and $p_j$ (both not $p_1$), the sign of the first determinant flips and the sign of the Vandermonde determinant also flips, so the numerator remains invariant under these swaps. Then, the numerator is expressible as a polynomial in the elementary symmetric polynomials in $p_2, \dots, p_k$ (let's call them $f_1^{(k-1)}, \dots, f_{k-1}^{(k-1)})$) and in particular, when we expand this polynomial in powers of $p_1$, each coefficient will be a symmetric polynomial in the other $p_i$s expressible in the $f_j^{(k-1)}$s. But, if $f_i^{(k)}$ is the $i$th elementary symmetric polynomial in all of the $p_i$s, then recall from our proof of the fundamental theorem of symmetric polynomials we can express each of the $f_j^{(k-1)}$s in terms of the $f_j^{(k)}$s and $p_1$, so the numerator is therefore a polynomial in the elementary symmetric polynomials of the $p_i$s and $p_1$, the former of which is expressible in the elementary symmetric polynomials of the $x_i$s. This completes the proof of Lagrange's Rational Function Theorem. 
\end{proof}

This gives us an immediate corollary/definition: 
\begin{corollary}
If we have a polynomial $p$ in $x_1, \dots, x_n$ such that applying all elements $\pi \in S_n$ to $p$ produces $n!$ different values, then any polynomial $q(x_1, \dots, x_n)$ is expressible as a rational function in the elementary symmetric polynomials of the $x_i$s and $p$.
\end{corollary}
This can be applied now to a specific set of roots $r_1, \dots, r_n$ of some polynomial equation: 
\begin{definition}
    A \textbf{Galois resolvent} of a polynomial with roots $r_1, \dots, r_n$ is a polynomial $p(x_1, \dots, x_n)$ such that over all $\pi \in S_n$, $\pi p(r_1, \dots, r_n)$ gives $n!$ distinct numerical results. 
\end{definition}
\begin{corollary}
The roots of a polynomial are expressible numerically as a rational function of the coefficients of that polynomial (i.e. elementary symmetric polynomials in the roots) and a Galois resolvent for that polynomial, if one exists. 
\end{corollary}
In particular, Lagrange's Rational Function Theorem gives us somewhat of a construction for how to go about expressing each root as a function of the coefficients of the polynomial and a Galois resolvent, if it exists. 

\begin{remark}
    Note that Galois resolvents have an additional constraint -- it's not enough for the $n!$ permutations acting on the resolvent to give different formal polynomials, there's a numerical dependence on the roots too! In particular, if a polynomial has repeated roots, this Galois resolvent can't exist. However, there's an easy way to test whether a polynomial $f(x)$ has a repeated root or not -- if $f(x)$ and $f'(x)$ have a nontrivial gcd (i.e. a polynomial that is linear or of higher degree), then that polynomial must divide both $f(x)$ and $f'(x)$, and so we can isolate those repeated roots! Therefore, we only really need to care about solving polynomials with all simple roots, as this derivative trick will allow us to handle any multiplicities. 
\end{remark}

\subsection{Existence of Galois Resolvents}
So now, it seems like the key to solving general polynomials through Lagrange's method is to be able to construct a Galois resolvent of a given polynomial equation. How do we do this? In particular, how do we know one exists? 

Galois was the first to realize that these so-called Galois resolvents always existed, and used them extensively in his work. In particular, he claimed that for any polynomial with distinct simple roots $r_1, \dots, r_n$, that a Galois resolvent exists in these roots that is a polynomial with integer coefficients. He never proved this though, so let's do it now. 

First, a lemma: 
\begin{lemma}
Suppose $p(x_1, \dots, x_n)$ is a nonzero polynomial in $n$ variables with rational coefficients. Then, we can find some $(x_1, \dots, x_n) = (a_1, \dots, a_n)$ for $a_i \in \ZZ$ such that $p(a_1, \dots, a_n) \neq 0$. 
\end{lemma}
\begin{proof}
We show this by induction on the number of variables. First, suppose we have a polynomial in one variable $n = 1$ with degree $d \geq 1$. Then, we claim among the integers in $[d+1]$ that for at least one $k \in [d+1]$ that $p(k) \neq 0$. For the sake of contradiction, if there was, then $p(x)$ would have to be identically zero. This gives us the existence of the integer we're looking for. 

Now, suppose we have a polynomial in $n$ variables, $p(x_1, \dots, x_n)$. Then, let's rewrite this polynomial so that we have a polynomial in $x_n$, but the coefficients are polynomials $p_i$ in $x_1, \dots, x_{n-1}$, so we have 
\[
    p(x_1, \dots, x_n) = p_0(x_1, \dots, x_{n-1}) + p_1(x_1, \dots, x_{n-1}) x_n + \dots + p_m(x_1, \dots, x_{n-1}) x^m
\]
where $m$ is the maximal degree of $x_n$ that appears. Now, we can pick integer values of $x_1, \dots, x_{n-1}$ so that $p_m$ is nonzero (we know it has to be a non-zero polynomial by construction). After plugging in these values of $x_1, \dots, x_{n-1}$, we now have a degree $m$ polynomial in one variable, after which we can simply apply the $n = 1$ argument for $x_n$. This completes the induction. 
\end{proof}
Now, let's construct a Galois resolvent. Suppose $r_1, \dots, r_n$ are the distinct roots of a degree $n$ polynomial with rational coefficients. We claim that for some integer coefficients $x_1, \dots, x_n$ that when taking $p(r_1, \dots, r_n) = \sum_{i = 1}^n x_i r_i$ and acting on it with $S_n$, that over all $\pi \in S_n$ we get different numerical results for $\pi p(r_1, \dots, r_n)$. Let's consider the following enormous product over all unordered pairs of permutations $\sigma, \tau \in S_n$: 
\[
    \Delta = \prod_{\braces{\sigma, \tau} \subseteq S_n} (\sigma p(r_1, \dots, r_n) - \tau p(r_1, \dots, r_n))^2 = \prod_{\braces{\sigma, \tau} \subseteq S_n} \left[ \sum_{i = 1}^n x_i (r_{\sigma(i)} - r_{\tau(i)})\right]^2
\]
Note that if we can find some set of values for $x_i$ such that $\Delta$ is nonzero, then we've constructed a Galois resolvent for the roots. Note that $\Delta$ is a nonzero polynomial in the variables $x_1, \dots, x_n$ by construction, and is also symmetric upon permuting the roots, so the coefficients of each term is expressible in the elementary symmetric polynomials of the roots, which are all rational by hypothesis. Then, our lemma above gives us a construction for an $n$-tuple of integers $a_1, \dots, a_n$ that make the $\Delta$ nonzero, so we are done. As such, Galois resolvents exist for any polynomial with rational coefficients. 

\subsection{How to Use a Galois Resolvent}
Now that we have a Galois resolvent for any polynomial with rational coefficients, and can construct it, let's see how we can use it to solve for our roots. 

Suppose we have a polynomial $f(x)$ with rational coefficients that has simple roots $r_1, \dots, r_n$ (unknown), and a Galois resolvent $p = \sum_{i=1}^n x_i r_i$ for integer $x_i$. Consider the set of all numbers that are expressible in terms of rational expressions (a quotient of polynomial expressions) in rationals or the roots $r_1, \dots, r_n$, $\QQ[r_1, \dots, r_n]$. Then, the corollary to Lagrange's Rational Function Theorem says that this is the same set of numbers as $\QQ[p]$, where we take the set of all numbers that are expressible as rational expressions in rationals or the Galois resolvent $p$. This allows us to tie the solutions of the original polynomial equation to the solutions of the resolvent equation, which we can solve. If we can solve the resolvent equation and adjoin its roots to $\QQ$, then this effectively constitutes a solution to the original equation, as the roots $r_i$ can be evaluated in terms of $p$. 

It feels like to some extent we have sort of made this ouroboros of a construction where we have reduced the solution of a general $n$th degree to a degree $n!$ polynomial, but this polynomial is often more symmetric and may sometimes be solvable in some cases. Galois' main contribution was to be able to characterize solutions constructed in this way by a Galois resolvent and answer algebraic questions about the nature of the solutions, especially if there were solutions to the original equations expressible in terms of radicals, the last standard algebraic operation we have not yet discussed so far. Gauss was able to express all roots of unity in terms of radicals, but Galois would be able to say when the roots of any $n$th degree polynomial were rationally expressible in radicals. 

Note that the construction for our Galois resolvent above works if our polynomial has coefficients in any reasonable set of numbers in which we can add and multiply -- $\QQ$ is a reasonable such set, but $\RR$ would be too, and so would $\CC$! All of these are examples of \textbf{fields}, and we are essentially talking about \textbf{extending} these fields to include the roots of polynomials with coefficients in these fields. At this point, we want to be able to say firm, formal things about these sets of ``reasonable numbers in which we can add and multiply,'' and since we know a thing or two about groups and rings (... right?) in which we already have a notion of both addition and multiplication, we should want to level up to fields, now armed with the specific goal in mind of understanding what's going on with this idea of ``extending a field to include a polynomial's roots.'' 


\section{Fields and Splitting Fields}
It's now unavoidable to have some formal understanding of fields in order to study the problem we wish. Up to this point, we've managed to avoid any explicit use of fields and stick to maybe just a little bit of group theory essentially, but in order to do the tricks we want to do as seen in the last section, we need a formal definition of a ``reasonable set of numbers in which we can do standard algebraic operations.'' So let's talk about fields! 


\section{Galois Groups}


\section{Groups of Solvable Equations}



\section{The Fundamental Theorem of Galois Theory}

\section{Aside -- Gauss and Roots of Unity}
We've been discussing the roots of unity when we introduced Lagrange resolvents, so we'll focus on them a little more now. Recall that $n$th roots of unity are of the form $\cos\left(\frac{2\pi k} n \right) + i \sin \left( \frac{2\pi k} n \right) = e^{\frac{2\pi k}n i}$ for $0 \leq k < n$. In order to manipulate these properly, we'd like to be able to compute these algebraically in terms of roots and standard algebraic operations. Define a \textit{primitive} $n$th root of unity to be a root of unity $e^{\frac{2\pi k}n i}$ such that for any $m < n$, $(e^{\frac{2\pi k}n i})^m \neq 1$. This is equivalent to saying that we want $\gcd(k, n) = 1$, as otherwise if $\gcd(k, n) = d > 1$, then $(e^{\frac{2\pi k}n i})^{\frac n d} = 1$ (and the converse is also true). 

Now, to reduce our work down, note that if $\gcd(a, b) = 1$ and $n = ab = \lcm(a, b)$, a primitive $n$th root of unity can be obtained by multiplying together a primitive $a$th root of unity and a primitive $b$th root of unity. This is some standard number theory fare -- note that $\omega$ and $\zeta$ are such roots of unity, then $\omega = e^{\frac{2\pi k}a i}$ and $\zeta = e^{\frac{2\pi l}b i}$, so $\omega \zeta = e^{2\pi i (\frac {kb + la}{ab})}$ with $\gcd(k, a) = 1$ and $\gcd(l, b) = 1$. Note then that $\gcd(kb + la, ab) = 1$ also, so this must be an $ab$th root of unity as well. 

As such, any $n$ can be broken into its prime powers, so it now suffices to find primitive $p^k$th roots of unity. However, a primitive $p^k$th root of unity is merely a $p^{k-1}$st root of a $p$th root of unity by the equivalent formulation of primitive roots of unity, so it suffices really to construct $p$th roots of unity algebraically. So let's do it!  

\subsection{$p$th Roots of Unity}
This will be initially a little bit of number theory to start. First, we need the existence of a primitive root mod $p$: 
\begin{lemma}[Existence of Primitive Roots mod $p$]
For every prime $p$, there is a positive integer $g$ such that every $0 < m < p-1$, $g^m \not\equiv 1 \mod p$, i.e. the smallest positive integer $k$ such that $g^k \equiv 1 \mod p$ is $p-1$. 
\end{lemma}
I don't really remember how to prove this or what the proof looks like, so I'll put a proof here so I'll remember at least one for the future: 
\begin{proof}
haha haven't done it yet
\end{proof}

\end{document}